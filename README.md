ğŸ“Š Learning an Unknown Probability Density Function using GANs
ğŸ“Œ Project Overview

This project demonstrates a data-driven approach to learning an unknown probability density function (PDF) using a Generative Adversarial Network (GAN).
Instead of assuming any analytical or parametric distribution (such as Gaussian or exponential), the PDF is learned purely from data samples.

The experiment is conducted on real-world NOâ‚‚ (Nitrogen Dioxide) concentration data from Indiaâ€™s air quality dataset. A nonlinear transformation is applied to the original feature, and a GAN is trained to model the resulting distribution implicitly.

This project highlights the use of GANs beyond image generation, specifically for probability density estimation from raw data alone.

ğŸ¯ Objectives

Transform a real-world feature using a nonlinear function

Assume no prior knowledge of the resulting distribution

Train a GAN to learn the distribution only from samples

Generate synthetic samples from the trained generator

Approximate the learned PDF using Kernel Density Estimation (KDE)

Compare the real and learned distributions visually

ğŸ“‚ Dataset Description

Dataset: India Air Quality Data

Source: Kaggle

Feature Used: NOâ‚‚ (Nitrogen Dioxide concentration)

Reason for Choosing NOâ‚‚:

Continuous-valued real-world data

Non-Gaussian behavior

Suitable for density learning tasks

The code automatically handles:

Unknown CSV file names

Non-UTF8 file encodings

Inconsistent column naming (e.g., "NO2", "NO2 (Âµg/m3)", etc.)

ğŸ”„ Data Transformation

Each NOâ‚‚ value 
ğ‘¥
x is transformed into a new variable 
ğ‘§
z using:

ğ‘§
=
ğ‘¥
+
ğ‘
ğ‘Ÿ
sin
â¡
(
ğ‘
ğ‘Ÿ
ğ‘¥
)
z=x+a
r
	â€‹

sin(b
r
	â€‹

x)

Where:

ğ‘
ğ‘Ÿ
=
0.5
Ã—
(
ğ‘Ÿ
â€Š
m
o
d
â€Š
7
)
a
r
	â€‹

=0.5Ã—(rmod7)
ğ‘
ğ‘Ÿ
=
0.3
Ã—
(
(
ğ‘Ÿ
â€Š
m
o
d
â€Š
5
)
+
1
)
b
r
	â€‹

=0.3Ã—((rmod5)+1)

ğ‘Ÿ
r is the university roll number

This nonlinear transformation ensures the resulting distribution has no known analytical form

ğŸ§  Why GANs?

Traditional density estimation methods often assume:

A known distribution family

A fixed parametric form

In this task:

The distribution of 
ğ‘§
z is unknown

No parametric PDF is allowed

GANs solve this by:

Learning distributions implicitly

Using adversarial training instead of likelihood estimation

Generating samples that resemble the true data distribution

ğŸ—ï¸ GAN Architecture
Generator

Input: Gaussian noise 
âˆ¼
ğ‘
(
0
,
1
)
âˆ¼N(0,1)

Output: Synthetic samples 
ğ‘§
ğ‘“
z
f
	â€‹


Architecture: Fully connected neural network with ReLU activations

Discriminator

Input: Real samples 
ğ‘§
z or generated samples 
ğ‘§
ğ‘“
z
f
	â€‹


Output: Probability of being real

Architecture: Fully connected neural network with Sigmoid output

Training Objective

Discriminator learns to distinguish real vs fake samples

Generator learns to fool the discriminator

ğŸ§ª Training Details

Framework: PyTorch

Loss Function: Binary Cross-Entropy

Optimizer: Adam

Epochs: 3000

Batch Size: 128

Stabilization:

Data standardization

Simple MLP architecture

Controlled learning rate

ğŸ“ˆ PDF Approximation

After training:

A large number of samples are generated from the generator

The probability density function is approximated using:

Kernel Density Estimation (KDE)

The estimated PDF is compared with the real transformed data PDF

This allows visual and qualitative evaluation of how well the GAN has learned the distribution.

ğŸ” Results & Observations
âœ… Mode Coverage

The GAN successfully captures the major modes of the transformed distribution

Minor modes are slightly smoothed, which is expected in KDE-based estimation

âœ… Training Stability

Training remains stable without mode collapse

Loss values converge smoothly over epochs

âœ… Quality of Generated Distribution

Strong overlap between real and GAN-generated PDFs

Small deviations observed in low-density tail regions

Overall shape and spread are well preserved

âš ï¸ Key Challenges Handled

Inconsistent column naming across datasets

Non-UTF8 encoded CSV files

Unknown dataset file paths in Kaggle

Real-world missing values

All issues are handled programmatically to ensure robustness and reproducibility.

ğŸš€ How to Run

Open the notebook in Kaggle

Attach the India Air Quality dataset

Run all cells

Observe:

GAN training logs

Final PDF comparison plot

No manual file path or column edits are required.

ğŸ§© Key Takeaways

GANs can be effectively used for probability density estimation

No analytical PDF assumption is required

Real-world data introduces challenges that must be handled carefully

Density learning is a powerful application of generative models beyond vision tasks

ğŸ“Œ Future Extensions

Use Wasserstein GAN (WGAN) for improved stability

Compare KDE with histogram-based density estimation

Extend to multivariate density learning

Apply to other environmental or sensor datasets

ğŸ‘¤ Author

Archit Arora
B.Tech Computer Engineering
Thapar Institute of Engineering and Technology
